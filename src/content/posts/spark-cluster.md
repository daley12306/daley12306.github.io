---
title: CÃ i Ä‘áº·t Spark Cluster trÃªn Hadoop
published: 2025-07-23
description: ''
image: ''
tags: [Hadoop, Spark, Big Data]
category: 'Data Engineering'
draft: false 
lang: 'vi'
---

## YÃªu cáº§u

TrÆ°á»›c khi cÃ i Ä‘áº·t, má»i ngÆ°á»i cáº§n pháº£i cÃ i Ä‘áº·t, thiáº¿t láº­p vÃ  káº¿t ná»‘i thÃ nh cÃ´ng cÃ¡c mÃ¡y master vÃ  slave láº¡i vá»›i nhau trÆ°á»›c nhÃ©.

---

## Cáº¥u hÃ¬nh há»‡ thá»‘ng

| Role     | IP Address       | Username          |
|----------|------------------|-------------------|
| Master   | `192.168.211.11` | `hadoopthienphuc` |
| Worker 1 | `192.168.211.13` | `hadoopthienphuc` |

---

## Táº£i vÃ  cÃ i Ä‘áº·t Spark

TrÃªn **mÃ¡y master**, thá»±c thi lá»‡nh sau Ä‘á»ƒ táº£i Spark vá» mÃ¡y. 

```bash
wget https://www.apache.org/dyn/closer.lua/spark/spark-3.5.6/spark-3.5.6-bin-hadoop3.tgz
```

Sau Ä‘Ã³ giáº£i nÃ©n vÃ  Ä‘á»•i láº¡i tÃªn thÆ° má»¥c thÃ nh `spark` Ä‘á»ƒ dá»… sá»­ dá»¥ng.

```bash
tar -xvf spark-3.5.6-bin-hadoop3.tgz
mv spark-3.5.6-bin-hadoop3 spark
```

---

## Copy thÆ° má»¥c Spark tá»« master sang slave

TrÃªn **mÃ¡y master**, thá»±c hiá»‡n lá»‡nh:

```bash
scp -r ~/spark hadoopthienphuc@192.168.211.13:~/
```

Lá»‡nh nÃ y sáº½ copy toÃ n bá»™ thÆ° má»¥c Spark tá»« master sang slave qua SSH.

---

## ThÃªm biáº¿n mÃ´i trÆ°á»ng Spark vÃ o `.bashrc`

TrÃªn **cáº£ master vÃ  slave**, má»Ÿ file `.bashrc`:

```bash
vi ~/.bashrc
```

ThÃªm Ä‘oáº¡n sau vÃ o cuá»‘i:

```bash
export SPARK_HOME=/home/hadoopthienphuc/spark
export PATH=$PATH:$SPARK_HOME/bin
export PATH=$PATH:$SPARK_HOME/sbin
export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.7-src.zip
```

Sau Ä‘Ã³ apply ngay:

```bash
source ~/.bashrc
```

---

## Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng Spark

Táº¡o file `spark-env.sh` náº¿u chÆ°a cÃ³:

```bash
cp $SPARK_HOME/conf/spark-env.sh.template $SPARK_HOME/conf/spark-env.sh
```

Má»Ÿ file cáº¥u hÃ¬nh:

```bash
vi $SPARK_HOME/conf/spark-env.sh
```

ThÃªm cáº¥u hÃ¬nh sau:

```bash
export SPARK_MASTER_HOST=192.168.211.11
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
export SPARK_WORKER_MEMORY=2g  # tuá»³ theo dung lÆ°á»£ng RAM mÃ¡y báº¡n
```

---

## Copy file cáº¥u hÃ¬nh sang mÃ¡y slave

Tá»« **mÃ¡y master**, cháº¡y:

```bash
scp $SPARK_HOME/conf/spark-env.sh hadoopthienphuc@192.168.211.13:$SPARK_HOME/conf/spark-env.sh
```

---

## Khai bÃ¡o mÃ¡y worker trong Spark

TrÃªn **mÃ¡y master**, chá»‰nh file `slaves` (hoáº·c `workers`):

```bash
vi $SPARK_HOME/conf/slaves
```

ThÃªm IP hoáº·c hostname cá»§a mÃ¡y slave:

```
192.168.211.13
```

> LÆ°u Ã½: Náº¿u báº¡n dÃ¹ng Spark báº£n má»›i hÆ¡n, file nÃ y cÃ³ thá»ƒ tÃªn lÃ  `workers`

---

## Khá»Ÿi Ä‘á»™ng Spark Cluster

Tá»« mÃ¡y master, cháº¡y:

```bash
$SPARK_HOME/sbin/start-all.sh
```

Lá»‡nh nÃ y sáº½:
- Khá»Ÿi Ä‘á»™ng Spark Master
- SSH sang slave vÃ  khá»Ÿi Ä‘á»™ng Spark Worker

---

## Kiá»ƒm tra giao diá»‡n Spark UI

Má»Ÿ trÃ¬nh duyá»‡t vÃ  truy cáº­p:

ğŸ”— [http://192.168.211.11:8080](http://192.168.211.11:8080)

Báº¡n sáº½ tháº¥y thÃ´ng tin:
- Spark Master Ä‘ang cháº¡y
- Worker Ä‘Æ°á»£c káº¿t ná»‘i (tá»« IP `192.168.211.13`)

